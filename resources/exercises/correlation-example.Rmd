---
title: "Example Correlation (Pearson's Product Moment Correlation Coefficient) Walkthrough"
#author: "Burrel Vann Jr"
output: 
  html_document: 
    css: markdown3.css
---


###### Navigation

* [**Intro to R**](Intro-and-Univariate-Statistics.html)
* [**t-Test**](t-test.html) [[*t-Test example*]](t-test-example.html)
* [**Analysis of Variance (ANOVA)**](anova.html) [[*ANOVA example*]](anova-example.html)
* [**Chi Square**](chi-square.html) [[*Chi Square example*]](chi-square-example.html)
* [**Correlation**](correlation.html) [[*Correlation example*]](correlation-example.html)

### Is there a relationship between a person's jealousy and their self-esteem? 

### The correlation

The correlation ($r$) or Pearson's product-moment correlation coefficient examines the relationship between two interval-ratio variables to see if the relationship reflects a true relationship that we could expect to find in the population. The test also tells us the **strength** (weak, moderate, strong) and **direction** (positive, negative) of that relationship. Rarely, we will see a non-relationship or a perfect relationship.

For this example, the correlation works perfectly because we're looking at how a person's self-esteem (a scale ranging from 1 -- low self-esteem to 50 -- high self-esteem), an interval-ratio level variable, is related to that person's level of jealousy (a scale ranging from 50 -- low jealousy to 150 -- high jealousy), another interval-ratio variable, to see if there is a true relationship that would exist in the population.



```{r, echo=F}
options(repos=c(CRAN="http://cran.stat.ucla.edu/"))
```

# 


```{r, results="hide", warning=FALSE, message=FALSE}
library(MASS)
library(psych)
library(lattice)
library(mosaic)
```

#


#### The Data

In total, we have 20 individuals. Below, show an image of their data in a spreadsheet and list their respective self-esteem scores and their jealousy scores. The data are as follows:

```{r, out.width = "200px", echo=FALSE}
knitr::include_graphics("est_jeal.png")
```


**Self-Esteem**: <span style="color:blue">`38, 32, 29, 36, 33, 36, 37, 32, 35, 26, 37, 34, 33, 33, 26, 30, 30, 40, 28, 28`</span> 

**Jealousy**: <span style="color:blue">`79, 104, 113, 94, 143, 88, 81, 48, 105, 115, 81, 87, 58, 113, 119, 111, 109, 108, 119, 133`</span>.

#

#### Reading in the Data

As in the [Intro to R](Intro-and-Univariate-Statistics.html) vignette, we can create an object out of a list of numbers using the concatenate <span style="color:blue">`c`</span> function.

Knowing that we have two variables: esteem score (interval-ratio independent variable) and jealousy score (interval-ratio dependent variable), we have to read in the variables separately (listing the values for each observation). To do so, we can use the following code:


```{r}
esteem <- c(38, 32, 29, 36, 33, 36, 37, 32, 35, 26, 37, 34, 33, 33, 26, 30, 30, 40, 28, 28)
jealousy <- c(79, 104, 113, 94, 143, 88, 81, 48, 105, 115, 81, 87, 58, 113, 119, 111, 109, 108, 119, 133)
```

Where the first observation in esteem corresponds with the number in the first observation of jealousy. For example, the first observation in the list for <span style="color:blue">`esteem`</span> is <span style="color:blue">`38`</span>, which corresponds with the first observation in the <span style="color:blue">`jealousy`</span> list, of <span style="color:blue">`79`</span>: This means the first observation is a person with an esteem score of 38 and a jealousy score of 79. 

Next, to appropriately prepare the data for analysis using the correlation, we have to merge the two lists. To merge the data, as in the [Intro to R](Intro-and-Univariate-Statistics.html) vignette, we can use the <span style="color:blue">`data.frame`</span> function. 

```{r}
data <- data.frame(esteem,jealousy)
```

Now we can call the data... 

```{r}
data
```




#### Assumptions and Diagnostics for Correlation

The assumptions for the Correlation are...

* Linearity
* Normality
* Absence of Range Restrictions
* Absence of Heterogeneous Subsamples

In addition, the previously-discussed assumptions for other tests (independence of observations) is implied, since all of these bivariate tests require random samples.

```{r, echo=FALSE}
data1 <- mtcars
```


##### 1. Linearity
* Variables move together in a linear fashion.

```{r}
plot(data$esteem, data$jealousy) #(x,y)
abline(lm(data$jealousy~data$esteem), col="Blue") # regression line (y~x))
```

Alternatively, we can add labels to the plot...

```{r}
plot(data$esteem, data$jealousy,
     main = "Scatterplot",
     xlab = "Self-Esteem Score",
     ylab = "Jealousy Score")
abline(lm(data$jealousy~data$esteem), col="Blue") # regression line (y~x))
```

  + <span style="color:red">Visual inspection of **scatterplot** to see if relationship is linear (straight-line). `Therefore, we meet the assumption of linearity`.</span> 

##### 2. Normality 

* Distributions must be relatively normal.
  + Visual inspection of...
    + Scatterplot (above)
    + Histograms for each variable
    + Box-and-Whiskers plot for each variable
    + Normality (Q-Q) plot for each variable

##### 2a. Histogram

```{r}
histogram(~ jealousy, data = data, main = "Histogram of 'Jealousy Score'", 
          type = "density", 
          panel=function(x, ...) {
            panel.histogram(x, ...)
            panel.abline(v=mean(x, na.rm = TRUE),col="red")
            panel.mathdensity(dmath=dnorm, col="black", 
                              args=list(mean=mean(x, na.rm = TRUE),
                                        sd=sd(x, na.rm = TRUE)), ...)            
          }) 
```

```{r}
histogram(~ esteem, data = data, main = "Histogram of 'Self-Esteem Score'", 
          type = "density", 
          panel=function(x, ...) {
            panel.histogram(x, ...)
            panel.abline(v=mean(x, na.rm = TRUE),col="red")
            panel.mathdensity(dmath=dnorm, col="black", 
                              args=list(mean=mean(x, na.rm = TRUE),
                                        sd=sd(x, na.rm = TRUE)), ...)            
          }) 
```

  + <span style="color:red">We can see from the histogram that for both variables, the data are moderately negatively skewed. It is safe to assume, however, that these data are close enough to normal to proceed with the statistical test.</span> 

##### 2b. Box-and-Whisker Plots

```{r}
bwplot(~ jealousy, data = data, pch="|", main = "Bar-and-Whisker Plot of 'Jealousy Score'")
```

```{r}
bwplot(~ esteem, data = data, pch="|", main = "Bar-and-Whisker Plot of 'Self-Esteem Score'")
```

  + <span style="color:red">We can see from the boxplot that for both variables, the data are somewhat normally distributed. Although, for the jealousy variable, the median is closer to the upper end (75$^{th}$ percentile) of the interquartile range than the middle. Despite this trend, it is safe to assume that these data are close enough to normal, since they aren't *drastically* different from normal, and therefore safe to proceed with the statistical test.</span>
  
##### 2c. Normality (Q-Q) Plots

```{r}
xqqmath(~ jealousy, data=data, fitline = TRUE, main = "Normal Q-Q Plot of 'Jealousy Score'", xlab = "Expected Normal")
```

```{r}
xqqmath(~ esteem, data=data, fitline = TRUE, main = "Normal Q-Q Plot of 'Self-Esteem Score'", xlab = "Expected Normal")
```

  + <span style="color:red">We can see from the Q-Q plot that for both variable, the data are somewhat normal, since there is no discernible pattern across the line (e.g. no strong curvilinear trend around normality line). It is therefore safe to proceed with the statistical test.</span> 

  + <span style="color:red">Based on the the three visual depictions above, the data seem normally-distributed. `Therefore, we meet the assumption of normality`.</span> 


##### 3. Absence of Range Restrictions
* Values on variables cannot be restricted to small range. Examine range for variables
  
```{r}
describe(data$jealousy)
```

```{r}
describe(data$esteem)
```

  + <span style="color:red">Here, it doesn't seem as though there are restictions on the possible ranges of self esteem or jealousy. `Therefore, we meet the assumption of absence of range restrictions`.</span> 

##### 4. Absence of Heterogeneous Subsamples
* Not having groups that have extremely different values (e.g. for which a t-test/ANOVA might appropriately identify). Examine various groups in the sample

  + <span style="color:red">Here. we don't different samples, since there is no identifying information for the cases. `Therefore, we meet the assumption of absence of heterogeneous subsamples`.</span> 


#
### The Correlation Test Calculation

The calculation for the Chi Square is:

 $r = \frac{\sum (X - \bar{X})(Y - \bar{Y})}{\sqrt{\sum (X - \bar{X})^2 \sum(Y - \bar{Y})^2}}$
 


In addition, the degrees of freedom ($df$) for the test is...<br> 
* $df = N - 2$


### Running the Correlation



For Correlation, within the <span style="color:blue">`cor.test`</span> function, the independent variable is listed first and the dependent variable is listed second. 



```{r}
cor.test(data1$wt, data1$mpg)
```

In the output above, we see the $r$-obtained value (-.8676594), the degrees of freedom (30), and the p-value (1.294 x $10^{-10}$ = .0000000001294), which is much less than our set alpha level of .05).

To interpret the findings, we report the following information:

* The test used
* If you **reject** or **fail to reject** the null hypothesis
* The variables used in the analysis
* The degrees of freedom, calculated value of the test ($r_{obtained}$), and $p-value$
  + $r(df) = r_{obtained}$, $p-value$

“Using the Pearson's correlation test ($r$), I reject/fail to reject the null hypothesis that there is no association between variable one and variable 2, in the population, $r(?) = ?, p ? .05$” 

  + <span style="color:red">“Using the Pearson's correlation test ($r$), I reject the null hypothesis that there is no association between the weight of a car and its fuel economy in terms of miles per gallon, in the population, $r = -.8676594, p \lt .05$. In particular, we have a *strong* *negative* relationship between car weight and car mpg, such that, as the weight of the car increases, the miles-per-gallon decreases.”</span>



<br><br><br>
